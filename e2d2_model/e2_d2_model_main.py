# -*- coding: utf-8 -*-
"""IDL_Team_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18besoG7RRthNJMDhwMPWD_cjw6tFWFyc
"""

import numpy as np

data = np.load("data_winlen10.npy")

print("Shape of data is {0}".format(data.shape))
print("Number of entries is {0}".format(data.shape[0]))
print("Number of timestamps per entry is {0}".format(data.shape[1]))
print("Number of variables per timestamp is {0}".format(data.shape[2]))

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
import os

train_data = np.load("train_data_winlen_10.npy")
test_data = np.load("val_data_winlen_10.npy")

train_data.shape

test_data.shape

#train_data = data[:1980, :, :]
#test_data = data[1980: ,: , :]
train_data = data[:1980, :, [0, 1, 2, 3, 4, 8, 9, 10, 11]]
test_data = data[1980: ,: , [0, 1, 2, 3, 4, 8, 9, 10, 11]]



train_data2 = data[:1980, :, :]
test_data2 = data[1980: ,: , :]

train_data2 = train_data[:, :, [0, 1, 2, 3, 4, 8, 9, 10, 11]]
test_data2 = test_data[:, :, [0, 1, 2, 3, 4, 8, 9, 10, 11]]

def split_series(data, past_length, future_length):
  X = list()
  y = list()
  for i in range(data.shape[0]):
    X.append(data[i, :past_length, :])
    y.append(data[i, (-1 * future_length): , :])
  return np.array(X), np.array(y)

past_length = 5
future_length = 5
train_data_X, train_data_y = split_series(train_data, past_length, future_length)
test_data_X, test_data_y = split_series(test_data, past_length, future_length)

train_data_X2, train_data_y2 = split_series(train_data2, past_length, future_length)
test_data_X2, test_data_y2 = split_series(test_data2, past_length, future_length)

test_data_X2.shape

n_features = 9
#tt = train_data_X.reshape((train_data_X.shape[0], train_data_X.shape[1],n_features))

encoder_inputs = tf.keras.layers.Input(shape=(past_length, n_features))
encoder_l1 = tf.keras.layers.LSTM(100,return_sequences = True, return_state=True)
encoder_outputs1 = encoder_l1(encoder_inputs)
encoder_states1 = encoder_outputs1[1:]
encoder_l2 = tf.keras.layers.LSTM(100, return_state=True)
encoder_outputs2 = encoder_l2(encoder_outputs1[0])
encoder_states2 = encoder_outputs2[1:]
#
decoder_inputs = tf.keras.layers.RepeatVector(future_length)(encoder_outputs2[0])
#
decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)
decoder_l2 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_l1,initial_state = encoder_states2)
decoder_outputs2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l2)
#
model_e2d2 = tf.keras.models.Model(encoder_inputs,decoder_outputs2)
#
model_e2d2.summary()

'''
encoder_inputs = tf.keras.layers.Input(shape=(past_length, n_features))
encoder_l1 = tf.keras.layers.LSTM(100,return_sequences = True, return_state=True)
encoder_outputs1 = encoder_l1(encoder_inputs)
encoder_states1 = encoder_outputs1[1:]
encoder_l2 = tf.keras.layers.LSTM(100, return_sequences=True,return_state=True)
encoder_outputs2 = encoder_l2(encoder_outputs1[0])
encoder_states2 = encoder_outputs2[1:]
encoder_l3 = tf.keras.layers.LSTM(100, return_state=True)
encoder_outputs3 = encoder_l3(encoder_outputs2[0])
encoder_states3 = encoder_outputs3[1:]
#
decoder_inputs = tf.keras.layers.RepeatVector(future_length)(encoder_outputs3[0])
#
decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)
decoder_l2 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_l1,initial_state = encoder_states2)
decoder_l3 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_l2,initial_state = encoder_states3)
decoder_outputs3 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l3)
#
model_e3d3 = tf.keras.models.Model(encoder_inputs,decoder_outputs3)
#
model_e3d3.summary()
'''

global_loss = list()
global_val_loss = list()

#reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)
model_e2d2.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())
history_e2d2=model_e2d2.fit(train_data_X,train_data_y,epochs=100,validation_data=(test_data_X,test_data_y),batch_size=32,verbose=1)#,callbacks=[reduce_lr])

history_losses = history_e2d2.history['loss']
history_val_losses = history_e2d2.history['val_loss']
for i in range(len(history_losses)):
  global_loss.append(history_losses[i])
  global_val_loss.append(history_val_losses[i])

import pickle
with open('global_loss.pkl', 'wb') as f:
  pickle.dump(global_loss, f)
with open('global_val_loss.pkl', 'wb') as f:
  pickle.dump(global_val_loss, f)

len(global_loss)

'''
reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)
model_e3d3.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())
history_e3d3=model_e3d3.fit(train_data_X,train_data_y,epochs=100,validation_data=(test_data_X,test_data_y),batch_size=32,verbose=1,callbacks=[reduce_lr])
'''

out = model_e2d2.predict(test_data_X)

np.save("out.npy", out)

np.save("test_data_9_features.npy", test_data_y)
np.save("test_data_all_features.npy", test_data_y2)

test_data_y2.shape

print(out[1])
print(test_data_y[1])

print(test_data_y2[1])

model_e2d2.save("e2_d2_model_600_latest")

!zip -r e2_d2_model_600_latest.zip e2_d2_model_600_latest

from keras import backend as K
K.set_value(model_e2d2.optimizer.learning_rate, 0.0000001)

!unzip e2_d2_model_600_latest.zip

import keras
x_model = keras.models.load_model('e2_d2_model_600_latest')

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.xlabel('Epochs')
plt.ylabel('Huber Loss')
plt.title('Train and Validation Losses across Epochs')
plt.plot([i for i in range(600)],global_loss, color='blue', linewidth = 1,  label = 'Train Loss')
plt.plot([i for i in range(600)], global_val_loss,color='red', linewidth = 1,  label = 'Validation Loss')
plt.legend()
plt.show()

model_e2d2.save("e2_d2_model_600_latest.h5", save_format="h5")

out = x_model.predict(test_data_X2)

out.shape

out[:, :, 5:8] = np.zeros((675, 5, 3))

out.shape

import numpy as np

# Create the original array with shape (675, 5, 9)
original_array = np.zeros((675, 5, 9))

# Create the new array with shape (675, 5, 12)
new_array = np.zeros((675, 5, 12))

# Create the array containing the elements to insert at indices 5, 6, 7
insert_array = np.ones((675, 5, 3))

# Insert the elements from the insert_array at indices 5, 6, 7 along the third axis of the new_array
new_array = np.insert(new_array, [5, 6, 7], insert_array, axis=2)

# Copy the original_array data to the first 5 columns of the new_array
new_array[:, :, :9] = original_array

new_array = np.zeros((675, 5, 12))

out[0][0]

test_data_y[0][0]

new_array = np.insert(out, 5, test_data_y[:, :, 5], axis=2)

new_array[0][0]

new_array = np.insert(new_array, 6, test_data_y[:, :, 6], axis=2)

new_array = np.insert(new_array, 7, test_data_y[:, :, 7], axis=2)

new_array[0][0]

test_data_y[0][0]

out2 = np.resize(out, (675, 5, 12))

out[0][0]

out[0][0]

test_data_y.shape

print(new_array.shape)
np.save('predicted.npy', new_array)

np.save('ground_truth.npy', test_data_y)

np.save('first_5_frames.npy', test_data_X)

print(new_array.shape)
print(test_data_y.shape)
print(test_data_X.shape)

